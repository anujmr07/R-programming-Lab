---
title: "irish.Exp-2"
output: html_document
date: "2025-09-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
X= vector("numeric", length=10)
for(i in 1:10){
  X[i] = rbinom(1,1,0.8)
}
X
```

```{r}
sm = sum(X)
if (sm > 5){
  print(1)
}else{
  print(0)
}
  
```

```{r}
#2.Implement  step  1  10000  times  and  evaluate  the  probability  of  1  as  majority output in 10000 trial
ccount=0
for(j in 1:10000){
  X= vector("numeric", length=10)
for(i in 1:10){
  X[i] = rbinom(1,1,0.8)
}
  sm = sum(X)
if (sm > 5){
  ccount =ccount+1
}
}

acc = ccount/10000
acc
```

```{r}
# 3. Train a Decision Tree model on IRIS Datasetusing “rpart” library in R. Use 120 samples  randomly  for  train  dataset  and  30  samples  for  test  set. Evaluate  train and test set classification accuracy.

library(rpart)
df = read.csv("iris.csv")
nr = nrow(df)
mclass = vector("numeric", length = nr)
for(i in 1:nr){
  if(df[i,5] == 'setosa'){
    mclass[i] =0
  }
  else if(df[i,5] == 'virginica'){
    mclass[i]= 1
  }else{
    mclass[i] = 2
  }
  }
df = cbind(df, mclass)
df
```

```{r}
s = sample(x=1:nr , size=0.2*nr, replace = FALSE)
train= df[s,-5]
test = df[-s,-5]
mfit = rpart(mclass~., data=train,method = "class")# dot -> all
library(rpart.plot)
rpart.plot(mfit)
```

```{r}
ypredtrain = predict(mfit,newdata = train, type = "class") 
ypredtest = predict(mfit,newdata = test, type = "class") 
trainacc =mean(train[,5]==ypredtrain) 
testacc =mean(test[,5]==ypredtest)

print(trainacc) 
print(testacc)
```

```{r}
ntrees=10
vtrainacc=vector("numeric" , length = ntrees)
vtestacc=vector("numeric" , length = ntrees)
forest = vector("list", length = ntrees)
for (i in 1:ntrees){
  s = sample(x=1:nr , size=0.2*nr, replace = FALSE)
  train= df[s,-5]
  test = df[-s,-5]
  mfit = rpart(mclass~., data=train,method = "class")
  forest[i] = list(mfit =mfit)
  ypredtrain=predict(mfit,newdata = train,type = "class")
  ypredtest =predict(mfit,newdata = test,type = "class")
  vtrainacc[i] = mean(train[,5]==ypredtrain)
  vtestacc[i] = mean(train[,5]==ypredtest)

}
print(vtrainacc)
print(vtestacc)
```

```{r}
sntrain = data.frame(c(1:30))
sntest = data.frame(c(1:120))
for(j in 1:ntrees){
  ypredtrain = predict(forest[[j]], newdata = train, type = "class")
  ypredtest = predict(forest[[j]], newdata = test, type = "class")
  sntrain = cbind(sntrain, ypredtrain)
  sntest = cbind(sntest, ypredtest)
}
```

```{r}
library(DescTools)
get_value = function(x){
  sm=Mode(x)
  if(length(sm)==1){
    return(sm)
  }else{
    return(sm[1])
  }
}
fypredtrain = apply(sntrain[,2:11],1,get_value)
fypredtest = apply(sntest[,2:11],1,get_value)
ftrainacc = mean(train[,5]==fypredtrain)
ftestacc = mean(test[,5]==fypredtest)
print(ftrainacc)
print(ftestacc)

```

```{r}

```

```{r}
library(rpart)

# Step 1: Define features and all unique pairs of 2
features <- c("sepal_length", "sepal_width", "petal_length", "petal_width")
comb_2 <- combn(features, 2, simplify = FALSE)

ntrees <- 6
vtrainacc <- vector("numeric", length = ntrees)
vtestacc <- vector("numeric", length = ntrees)
forest <- vector("list", length = ntrees)

nr <- nrow(df)

# Step 2: Loop over 6 unique feature pairs
for (i in 1:ntrees) {
  feat_subset <- comb_2[[i]]
  s <- sample(1:nr, size = 0.8 * nr, replace = FALSE)  # 80% training
  
  train <- df[s, c(feat_subset, "mclass")]
  test  <- df[-s, c(feat_subset, "mclass")]
  
  mfit <- rpart(mclass ~ ., data = train, method = "class")
  forest[[i]] <- list(mfit = mfit, features = feat_subset)
  
  ypredtrain <- predict(mfit, newdata = train, type = "class")
  ypredtest  <- predict(mfit, newdata = test, type = "class")
  
  vtrainacc[i] <- mean(train$mclass == ypredtrain)
  vtestacc[i]  <- mean(test$mclass == ypredtest)
}

# Step 3: Output results
print("Training Accuracies:")
print(vtrainacc)

print("Testing Accuracies:")
print(vtestacc)

# Optional: View which features each tree used
tree_features <- sapply(comb_2, function(x) paste(x, collapse = " + "))
names(vtrainacc) <- tree_features
names(vtestacc) <- tree_features

```

```{r}
print(mean(vtrainacc))
print(mean(vtestacc))
```
